<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anomaly Detection on Nakatsuka Shunsuke</title>
    <link>https://salty-vanilla.github.io/portfolio/categories/anomaly-detection/</link>
    <description>Recent content in Anomaly Detection on Nakatsuka Shunsuke</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2019 00:00:00 +0900</lastBuildDate>
    
	<atom:link href="https://salty-vanilla.github.io/portfolio/categories/anomaly-detection/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</title>
      <link>https://salty-vanilla.github.io/portfolio/post/memorizing_normality_to_detect_anomaly_memory-augmented_deep_autoencoder_for_unsupervised_anomaly_detection/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0900</pubDate>
      
      <guid>https://salty-vanilla.github.io/portfolio/post/memorizing_normality_to_detect_anomaly_memory-augmented_deep_autoencoder_for_unsupervised_anomaly_detection/</guid>
      <description>1. どんなもの？  Autoencoder（差分ベース）の異常検知モデル 潜在変数にMemory構造を導入することで正常データ以外も復元できてしまう”汎化”を防ぐ  2. 先行研究と比べてどこがすごい？  Autoencoderを使った異常検知では，モデルが汎化してしまい異常データまでも復元できてしまう問題があった 潜在変数にMemory構造を追加することで，正常データの分布内のデータしか復元できないようにした  3. 技術や手法の&amp;quot;キモ&amp;quot;はどこ？  Memory構造がキモ  全体の流れ  Encoderからまず$z$を得る $$ z = f_e(x; \theta_e) $$ メモリ構造を用いて$\hat{z}$を得る（後述） Decoderで$\hat{z}$から復元する  $$ \hat{x} = f_d(\hat{z}; \theta_d) $$
Memory   それぞれ変数を定義する
 $M \in \mathbb{R}^{N \times C}$: Memory行列 $m_i$: $M$の$i$行目Vector $N$: メモリ数 $C$: $\hat{z}$の次元数（論文内では$z$の次元数と一致） $w \in \mathbb{R}^{1 \times N} $: Attention Weight Vector    Encoderから得られた$z$と$m_i$の距離（内積）を算出して，softmaxすることで$w$を求める $$ w_i = \frac{\exp(d(z, m_i))}{\Sigma^N_{j=1}\exp(d(z, m_j))} $$</description>
    </item>
    
    <item>
      <title>Iterative energy-based projection on a normal data manifold for anomaly localization</title>
      <link>https://salty-vanilla.github.io/portfolio/post/iterative_energy-based_projection_on_a_normal_data_manifold_for_anomaly_localization/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0900</pubDate>
      
      <guid>https://salty-vanilla.github.io/portfolio/post/iterative_energy-based_projection_on_a_normal_data_manifold_for_anomaly_localization/</guid>
      <description>1. どんなもの？ Autoencoderベースの異常検知手法．Autoencoderの問題である画像内の一部の異常が画像全体の復元に影響を与えてしまい上手く異常部位をLocalicationできないという問題にタックル．
2. 先行研究と比べてどこがすごい？  Autoencoderベースのモデルでは，異常画像が入力された際に異常部位以外も再構成が崩れてしまい上手くLocalizationできないという問題があった また，Blurが発生してしまう 上記2点を繰り返し，$x$を更新していく方法で解決する  3. 技術や手法の&amp;quot;キモ&amp;quot;はどこ？   エネルギー関数は，再構成誤差($L_r$)と正則化項（更新しても原画像から離れすぎないようにする正則化） $$ E(x_t) = L_r(x_t) + \lambda|| x_t - x_0 || $$ $$ L_r(x_t) = \mathbb{E} [ | f_{VAE}(x_t) - x_t | ^r ] $$
  エネルギー関数を最小化するように，入力画像$x_0$を更新していく $$ x_{t+1} = x_t - \alpha \nabla_x E(x_t) $$
  再構成が大きい部位は更新量を大きく，小さい部位は小さくすればなお良し $$ x_{x+1} = x_t - \alpha ( \nabla_xE(x_t) \odot | f_{VAE}(x_t) - x_t | ^2 ) $$</description>
    </item>
    
  </channel>
</rss>