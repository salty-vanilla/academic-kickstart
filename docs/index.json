[{"authors":["admin"],"categories":null,"content":"2013年岐阜大学工学部 電気電子・情報工学科に入学．情報分野を専門領域として学ぶ． その中で，人工知能・コンピュータビジョンに興味を持ち，2015年より加藤研究室に所属する． 学部・修士課程を通して，ニューラルネットワーク・画像処理・異常検知の研究に従事．特に工業製品や食品における外観検査の自動化をテーマとして研究を行った． また2018年度より応用生物科学部と協同しニューラルネットワークを用いた野生動物検知の研究にも着手．\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://salty-vanilla.github.io/portfolio/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/portfolio/author/admin/","section":"author","summary":"2013年岐阜大学工学部 電気電子・情報工学科に入学．情報分野を専門領域として学ぶ． その中で，人工知能・コンピュータビジョンに興味を持ち，2015年より加藤研究室に所属する． 学部・修士課程を通して，ニューラルネットワーク・画像処理・異常検知の研究に従事．特に工業製品や食品における外観検査の自動化をテーマとして研究を行った． また2018年度より応用生物科学部と協同しニューラルネットワークを用いた野生動物検知の研究にも着手．","tags":null,"title":"Nakatsuka Shunsuke ","type":"author"},{"authors":null,"categories":["Representation Learning"],"content":"1. どんなもの？  表現学習の枠組み Denoising Autoencoderをベースにより良い特徴表現を獲得 具体的には，入力データにノイズを付加するのではなくLaplacian Pyramidのrandom level目でノイズを付加する   2. 先行研究と比べてどこがすごい？  通常のDAEより優れた特徴表現の獲得 他の表現学習とは異なり，domain assumptionやpseudo labelを必要としない  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  入力データにノイズを付加するのではなく，Laplacian Pyramidのlevel $l$層目でノイズを付加する $l$層目のLaplacian Pyramid $x_l^L$ は，$l$層目のGaussian Pyramid $x_l^G$ を用いて下式 $$ x_l^L = x_l^G - upsample(x_{l+1}^G) $$ $x_0^G$ は元画像に等しい．上式の $l$ を 0 に向かって繰り返し計算すると元画像が求められる $$ x_l^G = x_l^L + upsample(x_{l+1}^G) $$ この繰り返しの途中でノイズを付加する   algorithmは以下  $c \\in C$ はどんなノイズを付加するかのセットと要素 objectiveは通常のMSE     4. どうやって有効だと検証した？ MNIST  通常のDAEと比較して，input space, laplacian spaceどちらのnoiseに対しても正確に復元できていることがわかる 再構成Lossも低い   CIFAR10  再構成，画像検索共に通常のDAEより高精度   Imagenet  Supervised learningと同じようなconv filterが学習できている  conv層後の特徴を線形分類したときの精度比較．  LapDAEとAET-project[1]を組み合わせた LapDAE + Transが最高精度    Pascal VOCに転移学習しても最高精度   5. 議論はあるか？  単純な方法でより良い特徴表現の獲得に成功している objectiveはMSEのままなのに，blurが軽減されているのはなぜ？ Gaussian pyramidを作る際にGaussian filterはかけてる？  6. 次に読むべき論文はある？  Zhang, L., Qi, G.-J., Wang, L., \u0026amp; Luo, J. (2019). AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data. Retrieved from http://arxiv.org/abs/1901.04596  ","date":1579273200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579273200,"objectID":"20e60eb245d5a34503e17b8a28d01577","permalink":"https://salty-vanilla.github.io/portfolio/post/lapdae/","publishdate":"2020-01-18T00:00:00+09:00","relpermalink":"/portfolio/post/lapdae/","section":"post","summary":"1. どんなもの？  表現学習の枠組み Denoising Autoencoderをベースにより良い特徴表現を獲得 具体的には，入力データにノイズを付加するのではなくLaplacian Pyramidのrandom level目でノイズを付加する   2. 先行研究と比べてどこがすごい？  通常のDAEより優れた特徴表現の獲得 他の表現学習とは異なり，domain assumptionやpseudo labelを必要としない  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  入力データにノイズを付加するのではなく，Laplacian Pyramidのlevel $l$層目でノイズを付加する $l$層目のLaplacian Pyramid $x_l^L$ は，$l$層目のGaussian Pyramid $x_l^G$ を用いて下式 $$ x_l^L = x_l^G - upsample(x_{l+1}^G) $$ $x_0^G$ は元画像に等しい．上式の $l$ を 0 に向かって繰り返し計算すると元画像が求められる $$ x_l^G = x_l^L + upsample(x_{l+1}^G) $$ この繰り返しの途中でノイズを付加する   algorithmは以下  $c \\in C$ はどんなノイズを付加するかのセットと要素 objectiveは通常のMSE     4. どうやって有効だと検証した？ MNIST  通常のDAEと比較して，input space, laplacian spaceどちらのnoiseに対しても正確に復元できていることがわかる 再構成Lossも低い   CIFAR10  再構成，画像検索共に通常のDAEより高精度   Imagenet  Supervised learningと同じようなconv filterが学習できている  conv層後の特徴を線形分類したときの精度比較．  LapDAEとAET-project[1]を組み合わせた LapDAE + Transが最高精度    Pascal VOCに転移学習しても最高精度   5.","tags":null,"title":"Laplacian Denoising Autoencoder","type":"post"},{"authors":null,"categories":["GAN"],"content":"1. どんなもの？  StyleGANのver2 StyleGANの問題の問題を改善 FIDの向上に加えて，PPL: Perceptual Path Lengthも向上  2. 先行研究と比べてどこがすごい？  StyleGANの問題であった水滴状のノイズ，潜在変数を走査しても顔のパーツが自然に変化しないなどの問題を改善 Instance Normの見直し，Progressive Growingの見直し，PPLの導入  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Revisit Instance Norm  StyleGANでは雨粒状のノイズ (artifact)が現れていた 原因はAdaINの演算    NVIDIAの動画がわかりやすい     この原因は Instance Norm にあり\n INは各feature mapの平均と分散で正規化 絶対値が小さくてもスパイク状の分布のfeature mapがあるとartifactが出てしまう INを無くせば，artifactが出ないらしい    a. StyleGAN\nb. StyleGANの詳細\nc. INのartifactを考慮した形\n A(mapping networkの出力$f(z)$)，conv後のstdのみを使うように変更 B(noise image)のaddはBlockの外に出した  d. (c)のoperationをweight demodulationで簡易化\n AのAdaINではAのstdで割り算していた これをfeature mapに対して割り算するのではなく，convのweightに対して割り算することで等価の演算に $s$はAをaffineして得られたスケールベクトル，$w \\in \\mathbb{R} ^{{ch_{in}} \\times {ch_{out}} \\times {hw}}$はconvのweight $$ w_{ijk}^{\\prime} = s_i \\cdot w_{ijk} $$ $$ w_{ijk}^{\\prime\\prime} = \\frac{w_{ijk}^{\\prime}}{\\sqrt{\\Sigma_{i,k}{{w_{ijk}^{\\prime}}^2 + \\epsilon}}} $$ 入力が標準偏差1のrandom variableであることを仮定している．これは$\\sigma$割っていることと同義 $$ \\sigma_j = \\sqrt{\\Sigma_{i,k}{{w_{ijk}^{\\prime}}^2}} $$  Image quality and generator smoothness While Perceptual Path Length  潜在空間のPerceptual Path Length: PPLが小さい ⇔ 生成のQuality高い PPLを正則化項として追加する $$ \\mathbb{E}_{w,y \\sim N(0,\\mathbf{I})} ( ||\\mathbf{J_w^T y}|| - a)^2 $$ $$ \\mathbf{J_w^T y} = \\nabla_w(g(w) \\cdot y) $$   Lazy Reguralization  loss関数は，logistic lossと$R_1$[1] $R_1$は毎ミニバッチごとに算出しなくても，16ミニバッチごとくらいでいいよということ それがlazy  Revisiting Progressive Growing   StyleGANでは，顔のパーツが潜在変数の変化に追従しないという問題あり\n 画像では，顔の向きが変わっているのに口が変わっていない     これは，StyleGANのProgressive Growing構造によるもの\n 各resolutionのGを段階的に学習することで，Gのレイヤは高周波成分を出力するように その結果，GがShift invarianceを失ってしまう    代替の構造として以下の(b),(c)を使う\n Generatorは(b) Discriminatorは(c)    4. どうやって有効だと検証した？   全工夫の有効性は   weight demodulationの有効性は以下\n artifactが消えたのがわかる     PPLの有効性は以下\n PPLが小さくなっている     PGに替わる構造の有効性は以下\n GとDにskipとresidualを選んだのはこの表から     生成は以下   5. 議論はあるか？  PG構造なくしたのはGood．非常に簡潔になった AdaINによるartifactへの対処としてのweight modulationも簡潔 しかし，依然として学習時間はDGX-1で13days  6. 次に読むべき論文はある？  Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? CoRR, abs/1801.04406, 2018. 5, 10 Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proc. CVPR, 2018. 1, 2, 4, 10, 12, 16 Animesh Karnewar, Oliver Wang, and Raghu Sesha Iyengar. MSG-GAN: multi-scale gradient GAN for stable image syn- thesis. CoRR, abs/1903.06048, 2019. 6  ","date":1577631600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577631600,"objectID":"aae105df73d88d47c774b8b0e940e650","permalink":"https://salty-vanilla.github.io/portfolio/post/stylegan2/","publishdate":"2019-12-30T00:00:00+09:00","relpermalink":"/portfolio/post/stylegan2/","section":"post","summary":"1. どんなもの？  StyleGANのver2 StyleGANの問題の問題を改善 FIDの向上に加えて，PPL: Perceptual Path Lengthも向上  2. 先行研究と比べてどこがすごい？  StyleGANの問題であった水滴状のノイズ，潜在変数を走査しても顔のパーツが自然に変化しないなどの問題を改善 Instance Normの見直し，Progressive Growingの見直し，PPLの導入  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Revisit Instance Norm  StyleGANでは雨粒状のノイズ (artifact)が現れていた 原因はAdaINの演算    NVIDIAの動画がわかりやすい     この原因は Instance Norm にあり\n INは各feature mapの平均と分散で正規化 絶対値が小さくてもスパイク状の分布のfeature mapがあるとartifactが出てしまう INを無くせば，artifactが出ないらしい    a. StyleGAN\nb. StyleGANの詳細\nc. INのartifactを考慮した形\n A(mapping networkの出力$f(z)$)，conv後のstdのみを使うように変更 B(noise image)のaddはBlockの外に出した  d. (c)のoperationをweight demodulationで簡易化\n AのAdaINではAのstdで割り算していた これをfeature mapに対して割り算するのではなく，convのweightに対して割り算することで等価の演算に $s$はAをaffineして得られたスケールベクトル，$w \\in \\mathbb{R} ^{{ch_{in}} \\times {ch_{out}} \\times {hw}}$はconvのweight $$ w_{ijk}^{\\prime} = s_i \\cdot w_{ijk} $$ $$ w_{ijk}^{\\prime\\prime} = \\frac{w_{ijk}^{\\prime}}{\\sqrt{\\Sigma_{i,k}{{w_{ijk}^{\\prime}}^2 + \\epsilon}}} $$ 入力が標準偏差1のrandom variableであることを仮定している．これは$\\sigma$割っていることと同義 $$ \\sigma_j = \\sqrt{\\Sigma_{i,k}{{w_{ijk}^{\\prime}}^2}} $$  Image quality and generator smoothness While Perceptual Path Length  潜在空間のPerceptual Path Length: PPLが小さい ⇔ 生成のQuality高い PPLを正則化項として追加する $$ \\mathbb{E}_{w,y \\sim N(0,\\mathbf{I})} ( ||\\mathbf{J_w^T y}|| - a)^2 $$ $$ \\mathbf{J_w^T y} = \\nabla_w(g(w) \\cdot y) $$   Lazy Reguralization  loss関数は，logistic lossと$R_1$[1] $R_1$は毎ミニバッチごとに算出しなくても，16ミニバッチごとくらいでいいよということ それがlazy  Revisiting Progressive Growing   StyleGANでは，顔のパーツが潜在変数の変化に追従しないという問題あり","tags":null,"title":"Analyzing and Improving the Image Quality of StyleGAN","type":"post"},{"authors":null,"categories":["Time Series"],"content":"1. どんなもの？  音の生波形から，eventのclassificationを行う raw waveformを1D CNNで周波数解析し，得られたTransformed Imageを2D CNNで識別 training dataが少ない場合でも有効なMix-trainingを提案  2. 先行研究と比べてどこがすごい？  Audio ClassificationはGoogleのBottleneck featureを使った識別，Handcrafted featureを使った識別がBaselineだった Bottleneckは情報のlostが，Handcraftedは抽出の困難さが問題 end-to-endな周波数特徴の抽出，識別を可能に  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  キモは，周波数特徴を抽出する1D CNNとその特徴を識別する2D CNN  Raw-waveforms-based network 1D CNN  1D CNNで時間方向にdownsamplingをかけることで，周波数特徴を抽出 FFTをNNに任せてるイメージで，識別に適した周波数特徴を抽出してくれることを期待 最終的には，$C \\times 1 \\times T$ (channel, 1, time)のfeature mapをtransposeして，$1 \\times C \\times T$ (1, channel, time)の画像に  2D CNN  得られた周波数特徴画像を2D Convolution 勾配消失を防ぐため，multi-resolutionalなfeature mapからpredictionを出力 attentionつき（attentionは多分，以下の構造）   Avg Poolは多分GAP．．．？  Mix-training strategy  training dataが少ない場合に有効なMix-training．pretraining的な扱い 2つの入力を$\\alpha \\in (0, 1)$でblend  $$ \\tilde{x}_k = \\alpha x_i + (1-\\alpha) x_j $$\n $\\tilde{x}_k$に対応する教師ラベルは以下（$y_i$,$y_j$はmulti-hot label)  $$ \\tilde{y}_k = sign(y_i + y_j) $$\n loss関数は  $$ L=-\\frac{1}{K} \\sum_{k, n} \\left(1-\\tilde{y}_{k n}\\right) \\log \\left(1-t_{k n}\\right)+\\tilde{y}_{k n} \\log t_{k n} $$\n$$ \\mathbf{t_k} = f_\\theta (\\tilde{x}_k) = [ t_{k1}, t_{k2}, \u0026hellip;, t_{kN} ] $$\n Mix-trainingが終わった後にはmixしないデータでfine-tuning  4. どうやって有効だと検証した？  Audio SetでBaselineとの精度比較 精度的には負けてるが，pretrainingなしなのはgoodかも   mix-trainingの有効性確認も mix-upよりも高精度  5. 議論はあるか？  周波数特徴画像にFFTのような説明性はあるか？ mix-trainingの教師データの総和が1にならないが良いのか．．． stemのConv 1x7は妥当か？  6. 次に読むべき論文はある？  Yu, C., Barsim, K. S., Kong, Q., \u0026amp; Yang, B. (2018). Multi-level Attention Model for Weakly Supervised Audio Classification. Retrieved from http://arxiv.org/abs/1803.02353 Zhang, H., Cisse, M., Dauphin, Y. N., \u0026amp; Lopez-Paz, D. (2017). mixup: Beyond Empirical Risk Minimization. Retrieved from https://arxiv.org/abs/1710.09412  ","date":1577026800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577026800,"objectID":"06e15f630c9d2aa98227d3509e99fcb6","permalink":"https://salty-vanilla.github.io/portfolio/post/an_end-to-end_audio_classification_system_based_on_raw_waveforms_and_mix-training_strategy/","publishdate":"2019-12-23T00:00:00+09:00","relpermalink":"/portfolio/post/an_end-to-end_audio_classification_system_based_on_raw_waveforms_and_mix-training_strategy/","section":"post","summary":"1. どんなもの？  音の生波形から，eventのclassificationを行う raw waveformを1D CNNで周波数解析し，得られたTransformed Imageを2D CNNで識別 training dataが少ない場合でも有効なMix-trainingを提案  2. 先行研究と比べてどこがすごい？  Audio ClassificationはGoogleのBottleneck featureを使った識別，Handcrafted featureを使った識別がBaselineだった Bottleneckは情報のlostが，Handcraftedは抽出の困難さが問題 end-to-endな周波数特徴の抽出，識別を可能に  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  キモは，周波数特徴を抽出する1D CNNとその特徴を識別する2D CNN  Raw-waveforms-based network 1D CNN  1D CNNで時間方向にdownsamplingをかけることで，周波数特徴を抽出 FFTをNNに任せてるイメージで，識別に適した周波数特徴を抽出してくれることを期待 最終的には，$C \\times 1 \\times T$ (channel, 1, time)のfeature mapをtransposeして，$1 \\times C \\times T$ (1, channel, time)の画像に  2D CNN  得られた周波数特徴画像を2D Convolution 勾配消失を防ぐため，multi-resolutionalなfeature mapからpredictionを出力 attentionつき（attentionは多分，以下の構造）   Avg Poolは多分GAP．．．？  Mix-training strategy  training dataが少ない場合に有効なMix-training．pretraining的な扱い 2つの入力を$\\alpha \\in (0, 1)$でblend  $$ \\tilde{x}_k = \\alpha x_i + (1-\\alpha) x_j $$","tags":null,"title":"An End-to-End Audio Classification System based on Raw Waveforms and Mix-Training Strategy","type":"post"},{"authors":null,"categories":["Normalization"],"content":"1. どんなもの？  pixelごとにチャネル方向に串刺しにして正規化する系の正規化手法 Encoder-Decoder構造（Domain transferなど）に適用すると良い生成  2. 先行研究と比べてどこがすごい？  BN，LN，INなどとは違って，空間解像度を保った正規化なのでstructuralな情報が残せる もちろん収束は早くなるし，安定もする  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Positional Normalization  feature mapの各Pixel（position)ごとにチャネル方向にstaticsを求める つまり，staticsのshapeは（b, h, w)  $$ \\mu_{b, h, w}=\\frac{1}{C} \\sum_{c=1}^{C} X_{b, c, h, w}, \\quad \\sigma_{b, h, w}=\\sqrt{\\frac{1}{C} \\sum_{c=1}^{C}\\left(X_{b, c, h, w}^{2}-\\mu_{b, h, w}\\right)+\\epsilon} $$\n$$ X_{b, c, h, w}^{\\prime}=\\gamma\\left(\\frac{X_{b, c, h, w}-\\mu}{\\sigma}\\right)+\\beta $$\n VGGにponoを差し込んでみると，画像の構造をstaticsが捉えているように見える  ただDenseNetでは，map端に望まない反応が見られる   Moment Shortcut  Encoder-Decoder構造において，Encoderのponoで得られたstd $\\sigma$を$\\gamma$，mean $\\mu$を$\\beta$として $$ x\u0026rsquo; = \\gamma x + \\beta $$ CycleGANやPix2Pixで有効 $\\mu$,$\\sigma$に対して，convして，$\\beta,\\gamma \\in \\mathbb{R}^{B \\times H \\times W \\times C}$にしてからAffineするDynamic Moment Shortcutも提案  4. どうやって有効だと検証した？  Domain transfer (Map \u0026lt;-\u0026gt; Photo, Horse \u0026lt;-\u0026gt; Zebra)で実験 CycleGAN (baseline)を上回るのはもちろん，SPADEにも勝っている parameter数も少ない  5. 議論はあるか？  情報量的には軽量版Unetと感じた Unetはパラメータ数，計算量も格段に多くなるのでGood MUNITはlatent spaceにたどり着かないかもしれない情報がでるけど，大丈夫なのか  6. 次に読むべき論文はある？ ","date":1576508400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576508400,"objectID":"80400532f5ef19e07cb1962865a07008","permalink":"https://salty-vanilla.github.io/portfolio/post/pono/","publishdate":"2019-12-17T00:00:00+09:00","relpermalink":"/portfolio/post/pono/","section":"post","summary":"1. どんなもの？  pixelごとにチャネル方向に串刺しにして正規化する系の正規化手法 Encoder-Decoder構造（Domain transferなど）に適用すると良い生成  2. 先行研究と比べてどこがすごい？  BN，LN，INなどとは違って，空間解像度を保った正規化なのでstructuralな情報が残せる もちろん収束は早くなるし，安定もする  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Positional Normalization  feature mapの各Pixel（position)ごとにチャネル方向にstaticsを求める つまり，staticsのshapeは（b, h, w)  $$ \\mu_{b, h, w}=\\frac{1}{C} \\sum_{c=1}^{C} X_{b, c, h, w}, \\quad \\sigma_{b, h, w}=\\sqrt{\\frac{1}{C} \\sum_{c=1}^{C}\\left(X_{b, c, h, w}^{2}-\\mu_{b, h, w}\\right)+\\epsilon} $$\n$$ X_{b, c, h, w}^{\\prime}=\\gamma\\left(\\frac{X_{b, c, h, w}-\\mu}{\\sigma}\\right)+\\beta $$\n VGGにponoを差し込んでみると，画像の構造をstaticsが捉えているように見える  ただDenseNetでは，map端に望まない反応が見られる   Moment Shortcut  Encoder-Decoder構造において，Encoderのponoで得られたstd $\\sigma$を$\\gamma$，mean $\\mu$を$\\beta$として $$ x\u0026rsquo; = \\gamma x + \\beta $$ CycleGANやPix2Pixで有効 $\\mu$,$\\sigma$に対して，convして，$\\beta,\\gamma \\in \\mathbb{R}^{B \\times H \\times W \\times C}$にしてからAffineするDynamic Moment Shortcutも提案  4.","tags":null,"title":"POSITIONAL NORMALIZATION","type":"post"},{"authors":null,"categories":["GAN"],"content":"1. どんなもの？  GANのIS，FIDを向上させる系の論文 BigGANベースに大きなアーキテクチャの変更なしに高精度な生成．  2. 先行研究と比べてどこがすごい？  ベースはBigGAN 潜在変数をDiscriminatorが騙されやすいように更新した後，パラメータを更新することでhigh qualityとdiversityを実現  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  キモは，潜在変数をDiscriminatorが騙されやすいように更新した後，パラメータを更新すること  Latent Optimisation  潜在変数をDiscriminatorが騙されやすいように更新した後，パラメータを更新する $$ \\Delta z = \\alpha \\frac{\\partial f(z)}{\\partial z} $$ $$ z\u0026rsquo; = z + \\Delta z $$ ここで，f(z)は$z$をGeneratorに入力し得られたデータをDiscriminatorに与えることで得られる出力  Natural Gradient Descent  更新する$z$の空間はユークリッド空間でないことが多い． 通常の勾配法ではうまく更新できないことがある． 自然勾配法を用いて$z$を更新する．  $$ \\Delta z = \\alpha F^{-1} \\frac{\\partial f(z)}{\\partial z} = \\alpha F^{-1}g $$\n ここで，$F$はフィッシャー情報行列 $F$の算出はcost大なので，近似すると($\\beta$はハイパラの定数)  $$ F\u0026rsquo; = g \\cdot g^T + \\beta I $$\n$$ \\Delta z=\\alpha\\left(\\frac{I}{\\beta}-\\frac{g g^{T}}{\\beta^{2}+\\beta g^{T} g}\\right) g=\\frac{\\alpha}{\\beta}\\left(1-\\frac{|g|^{2}}{\\beta+|g|^{2}}\\right) g $$\n4. どうやって有効だと検証した？ Imagenetの生成で実験． baseline(a)よりLOGAN(b)の方がdiversityのある生成ができている． 5. 議論はあるか？  $z$を更新するだけでここまで精度が上がるのは驚き ただ，baselineがBigGANなので庶民には手が出せない dynamicの話とかappendixについては，まだ見れてない  6. 次に読むべき論文はある？ ","date":1575903600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575903600,"objectID":"c40cd8a18661325b199914c865982a3a","permalink":"https://salty-vanilla.github.io/portfolio/post/logan/","publishdate":"2019-12-10T00:00:00+09:00","relpermalink":"/portfolio/post/logan/","section":"post","summary":"1. どんなもの？  GANのIS，FIDを向上させる系の論文 BigGANベースに大きなアーキテクチャの変更なしに高精度な生成．  2. 先行研究と比べてどこがすごい？  ベースはBigGAN 潜在変数をDiscriminatorが騙されやすいように更新した後，パラメータを更新することでhigh qualityとdiversityを実現  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  キモは，潜在変数をDiscriminatorが騙されやすいように更新した後，パラメータを更新すること  Latent Optimisation  潜在変数をDiscriminatorが騙されやすいように更新した後，パラメータを更新する $$ \\Delta z = \\alpha \\frac{\\partial f(z)}{\\partial z} $$ $$ z\u0026rsquo; = z + \\Delta z $$ ここで，f(z)は$z$をGeneratorに入力し得られたデータをDiscriminatorに与えることで得られる出力  Natural Gradient Descent  更新する$z$の空間はユークリッド空間でないことが多い． 通常の勾配法ではうまく更新できないことがある． 自然勾配法を用いて$z$を更新する．  $$ \\Delta z = \\alpha F^{-1} \\frac{\\partial f(z)}{\\partial z} = \\alpha F^{-1}g $$\n ここで，$F$はフィッシャー情報行列 $F$の算出はcost大なので，近似すると($\\beta$はハイパラの定数)  $$ F\u0026rsquo; = g \\cdot g^T + \\beta I $$","tags":null,"title":"LOGAN: Latent Optimisation for Generative Adversarial Networks","type":"post"},{"authors":null,"categories":["Anomaly Detection"],"content":"1. どんなもの？  Autoencoder（差分ベース）の異常検知モデル 潜在変数にMemory構造を導入することで正常データ以外も復元できてしまう”汎化”を防ぐ  2. 先行研究と比べてどこがすごい？  Autoencoderを使った異常検知では，モデルが汎化してしまい異常データまでも復元できてしまう問題があった 潜在変数にMemory構造を追加することで，正常データの分布内のデータしか復元できないようにした  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  Memory構造がキモ  全体の流れ  Encoderからまず$z$を得る $$ z = f_e(x; \\theta_e) $$ メモリ構造を用いて$\\hat{z}$を得る（後述） Decoderで$\\hat{z}$から復元する  $$ \\hat{x} = f_d(\\hat{z}; \\theta_d) $$\nMemory   それぞれ変数を定義する\n $M \\in \\mathbb{R}^{N \\times C}$: Memory行列 $m_i$: $M$の$i$行目Vector $N$: メモリ数 $C$: $\\hat{z}$の次元数（論文内では$z$の次元数と一致） $w \\in \\mathbb{R}^{1 \\times N} $: Attention Weight Vector    Encoderから得られた$z$と$m_i$の距離（内積）を算出して，softmaxすることで$w$を求める $$ w_i = \\frac{\\exp(d(z, m_i))}{\\Sigma^N_{j=1}\\exp(d(z, m_j))} $$\n  $$ d(z, m_i) = \\frac{zm_i^T}{|z||m_i|} $$\n $\\hat{z}$を求める $$ \\hat{z} = wM = \\Sigma^N_{i=1}w_im_i $$  Hard Shrinkage for Sparse Addressing 上述のMemory構造でも復元できてしまう異常サンプルは出てくるので，$w$をスパースにすることでより制限する\n\\[ \\hat{w}_i = \\begin{cases} w_i \u0026amp; \\text{ if } w_i \u0026gt; \\lambda \\\\ 0 \u0026amp; \\text{ otherwise } \\end{cases} \\]\nObjective 再構成誤差と$\\hat{w}$そスパースにするための誤差の重み付き和 $$ L(\\theta_e, \\theta_d, M) = \\frac{1}{T} \\Sigma^T_{t=1}[R(x^t, \\hat{x}^t) + \\alpha E(\\hat{w}^t)] $$\n$$ R(x^t, \\hat{x}^t) = |x^t - \\hat{x}^t| ^2 $$\n$$ E(\\hat{w}^t) = \\Sigma^T_{i=1}-\\hat{w}^t\\log{\\hat{w}^t} $$\n論文内では,$\\alpha = 0.0002$\n4. どうやって有効だと検証した？ 画像では，MNIST・Cifar10で実験 動画では，UCSD-Ped2・CUHK・ShanghaiTechで実験 5. 議論はあるか？  汎化にスポット当てた論文でgood MVTec で実験してみたい  6. 次に読むべき論文はある？ ","date":1575903600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575903600,"objectID":"8897429e0892f9f34304db2a3f79d713","permalink":"https://salty-vanilla.github.io/portfolio/post/memorizing_normality_to_detect_anomaly_memory-augmented_deep_autoencoder_for_unsupervised_anomaly_detection/","publishdate":"2019-12-10T00:00:00+09:00","relpermalink":"/portfolio/post/memorizing_normality_to_detect_anomaly_memory-augmented_deep_autoencoder_for_unsupervised_anomaly_detection/","section":"post","summary":"1. どんなもの？  Autoencoder（差分ベース）の異常検知モデル 潜在変数にMemory構造を導入することで正常データ以外も復元できてしまう”汎化”を防ぐ  2. 先行研究と比べてどこがすごい？  Autoencoderを使った異常検知では，モデルが汎化してしまい異常データまでも復元できてしまう問題があった 潜在変数にMemory構造を追加することで，正常データの分布内のデータしか復元できないようにした  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？  Memory構造がキモ  全体の流れ  Encoderからまず$z$を得る $$ z = f_e(x; \\theta_e) $$ メモリ構造を用いて$\\hat{z}$を得る（後述） Decoderで$\\hat{z}$から復元する  $$ \\hat{x} = f_d(\\hat{z}; \\theta_d) $$\nMemory   それぞれ変数を定義する\n $M \\in \\mathbb{R}^{N \\times C}$: Memory行列 $m_i$: $M$の$i$行目Vector $N$: メモリ数 $C$: $\\hat{z}$の次元数（論文内では$z$の次元数と一致） $w \\in \\mathbb{R}^{1 \\times N} $: Attention Weight Vector    Encoderから得られた$z$と$m_i$の距離（内積）を算出して，softmaxすることで$w$を求める $$ w_i = \\frac{\\exp(d(z, m_i))}{\\Sigma^N_{j=1}\\exp(d(z, m_j))} $$","tags":null,"title":"Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection","type":"post"},{"authors":null,"categories":["Anomaly Detection","VAE"],"content":"1. どんなもの？ Autoencoderベースの異常検知手法．Autoencoderの問題である画像内の一部の異常が画像全体の復元に影響を与えてしまい上手く異常部位をLocalicationできないという問題にタックル．\n2. 先行研究と比べてどこがすごい？  Autoencoderベースのモデルでは，異常画像が入力された際に異常部位以外も再構成が崩れてしまい上手くLocalizationできないという問題があった また，Blurが発生してしまう 上記2点を繰り返し，$x$を更新していく方法で解決する  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？   エネルギー関数は，再構成誤差($L_r$)と正則化項（更新しても原画像から離れすぎないようにする正則化） $$ E(x_t) = L_r(x_t) + \\lambda|| x_t - x_0 || $$ $$ L_r(x_t) = \\mathbb{E} [ | f_{VAE}(x_t) - x_t | ^r ] $$\n  エネルギー関数を最小化するように，入力画像$x_0$を更新していく $$ x_{t+1} = x_t - \\alpha \\nabla_x E(x_t) $$\n  再構成が大きい部位は更新量を大きく，小さい部位は小さくすればなお良し $$ x_{x+1} = x_t - \\alpha ( \\nabla_xE(x_t) \\odot | f_{VAE}(x_t) - x_t | ^2 ) $$\n  つまるところ，学習済みのVAEを用意して，テストデータを繰り返し入力・更新して元のManifoldにより近づけるイメージ （近いモデルはAnoGAN）\n  4. どうやって有効だと検証した？   MVTECに対して，実験\n  それぞれ ***-gradが提案手法   通常のAutoencoderより，適切に異常部位のLocalizationができていることを確認   5. 議論はあるか？  iterativeにすることで推論時間はどのくらいになる？ AnoGANと似たようなmethodだが，比較は？  6. 次に読むべき論文はある？  Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. CoRR, abs/1903.05789, 2019. Ian  ","date":1575817200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575817200,"objectID":"2c63b045ec3d2c60a3dba4a7d4959a99","permalink":"https://salty-vanilla.github.io/portfolio/post/iterative_energy-based_projection_on_a_normal_data_manifold_for_anomaly_localization/","publishdate":"2019-12-09T00:00:00+09:00","relpermalink":"/portfolio/post/iterative_energy-based_projection_on_a_normal_data_manifold_for_anomaly_localization/","section":"post","summary":"1. どんなもの？ Autoencoderベースの異常検知手法．Autoencoderの問題である画像内の一部の異常が画像全体の復元に影響を与えてしまい上手く異常部位をLocalicationできないという問題にタックル．\n2. 先行研究と比べてどこがすごい？  Autoencoderベースのモデルでは，異常画像が入力された際に異常部位以外も再構成が崩れてしまい上手くLocalizationできないという問題があった また，Blurが発生してしまう 上記2点を繰り返し，$x$を更新していく方法で解決する  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？   エネルギー関数は，再構成誤差($L_r$)と正則化項（更新しても原画像から離れすぎないようにする正則化） $$ E(x_t) = L_r(x_t) + \\lambda|| x_t - x_0 || $$ $$ L_r(x_t) = \\mathbb{E} [ | f_{VAE}(x_t) - x_t | ^r ] $$\n  エネルギー関数を最小化するように，入力画像$x_0$を更新していく $$ x_{t+1} = x_t - \\alpha \\nabla_x E(x_t) $$\n  再構成が大きい部位は更新量を大きく，小さい部位は小さくすればなお良し $$ x_{x+1} = x_t - \\alpha ( \\nabla_xE(x_t) \\odot | f_{VAE}(x_t) - x_t | ^2 ) $$","tags":null,"title":"Iterative energy-based projection on a normal data manifold for anomaly localization","type":"post"},{"authors":null,"categories":null,"content":"   はじめに 工業製品の製造工程において，出荷製品の品質安定化のために検査は必要不可欠である． とくに外観検査工程では人による目視検査が主であるが，検査員毎の判断基準のばらつきが長年の課題であった． 近年では深層学習技術の進歩により，これまで困難とされてきた目視検査工程でも自動化が検討されている． 従来の深層学習では正常品と異常品を識別するために大量のサンプルが必要であるが，現実には大量の異常品を確保することは困難である． そのため，正常品のみ，もしくは正常品と少数の異常品から良否識別できる枠組みが求められている． また，画像検査においては，1枚の異常画像の中に正常な領域と異常な領域が混在していることがある． このとき，異常な領域が占める割合が大きければ，検出は容易であるが，小さい場合は難しい． また，異常には様々な種類が存在し，それぞれを検出するのに適した解像度が存在するはずである． そのため，本稿ではComplementary GANに Multi-scale Patch の枠組みを加えたモデルに正常品のみを学習させ，正常分布とその補集合分布をモデリングし異常検知を行う手法を提案する．\n従来のNNを使った異常検知 差分ベース  Autoencoder AnoGAN ADGAN  メリット  差分ベースなので，欠陥箇所のLocalizationが可能 学習が容易  デメリット  外観検査においては差分の出づらい欠陥が存在する Blurが発生し，高周波成分が差分として現れてしまう    潜在変数ベース  Flow-based Model Adversarial Autoencoder  メリット  異常度を対数尤度としてダイレクトに算出できる 潜在変数による低次元データの可視化が可能  デメリット  欠陥情報が消失してしまう Out of distributionのデータでも尤度が高くなってしまうことがある．    Complementary GAN そもそもGANとは？  GeneratorとDiscriminatorの2つのNetworkを持つ Generatorは，Discriminatorを騙すように本物に近いデータを生成する Discriminatorは，入力が本物のデータなのか・Generatorによって生成された偽のデータなのかを識別する GeneratorとDiscriminatorが↑の学習をすることで，Generatorは本物に近いデータを生成できるようになる．  結局GANは何を学習している？  Discriminatorは生成分布とデータ分布の”離れ度合い”を測るDivergence Estimator Generatorは算出された”離れ度合い”を最小化する その結果，生成分布とデータ分布が近づいていき，本物に近い画像が生成できる  Discriminatorの出力って異常検知に使える？  Discriminatorは，本物と偽物が見極められるので，本物を正常データとすれば，正常/異常が分別できるのでは！？という考え それは難しい Discriminatorが識別しているのは，本物のデータであるか偽物のデータであって，偽物データの中に異常分布の要素は全く含まれていない  GANとComplementary GANの違いは？  GANはデータ分布と生成分布を測り，近づける Complementary GANはデータ分布と生成分布を測り，データ分布の補集合分布と生成分布を近づける つまり，Complementary GANは正常データには存在しないデータを生成し，Discriminatorは正常と正常ではないの識別境界となる  Multi-scale Patch Discriminator  外観検査では，欠陥の大きさは様々 CNNで，Conv + Poolingを積み重ねていくと小さい欠陥の情報は消えてしまう かといって，浅すぎるCNNでは識別はできない  そこで，Discriminatorの出力を[0, 1]のスカラーではなく，正常度MAPとすることで↑の問題に対処する\n実験 LEDチップ画像 サンプル データ内訳     良品 不良品     training 70000 0   test 10000 204    生成された補集合画像  不良品画像とは一致しないが，良品画像には近いが良品ではないものが生成できていることを確認    Discriminatorによる異常度MAP  赤に近いほど異常度が大きく，青に近いほど異常度が低い 各行左から，入力・8x8 MAP・4x4 MAP・2x2 MAP 8x8 MAPでは小さい欠陥が，2x2 MAPでは面積の大きい欠陥が検出される    精度   Reference  Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Nets. Advances in Neural Information Processing Systems, 2672–2680. Zheng, P., Yuan, S., Wu, X., Li, J., \u0026amp; Lu, A. (2018). One-Class Adversarial Nets for Fraud Detection. ArXiv Preprint ArXiv:1803.01798. Bergmann, P., Fauser, M., Sattlegger, D., \u0026amp; Steger, C. (2019). MVTec AD \u0026ndash; A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Karras, T., Laine, S., \u0026amp; Aila, T. (2018). A Style-Based Generator Architecture for Generative Adversarial Networks. Retrieved from https://arxiv.org/abs/1812.04948  ","date":1575126000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575126000,"objectID":"92ac6e90d5886054b39de0842a0ca6ac","permalink":"https://salty-vanilla.github.io/portfolio/project/visual_inspection/","publishdate":"2019-12-01T00:00:00+09:00","relpermalink":"/portfolio/project/visual_inspection/","section":"project","summary":"工業部品や食品の外観検査をニューラルネットワークによって自動化","tags":null,"title":"Visual Inspection","type":"project"},{"authors":null,"categories":null,"content":"1. どんなもの？ Metric Learningの論文．分類をして，各クラス内の分散を小さく，クラス間の分散を大きくする系のMetric Learining．\n2. 先行研究と比べてどこがすごい？  クラス分類モデルのSoftmaxを少し改良するだけで適用できる ArcFaceと先行研究のSpehereFace・CosFaseのLoss関数は似ていて，それを一般化している  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Architecture 全体的な流れとしては， Base Block（VGGとかResNetとか）から特徴ベクトルを出力\n\\[ x' = f(x) \\]\n出力された特徴ベクトルをL2正則化\n\\[ x'' = \\frac{x'}{|x'|^2} \\]\n全結合層の重みをL2正則化\n\\[ w' = \\frac{w}{|w|^2} \\]\n正則化された特徴ベクトルと重みを内積（これがcosの値）\n\\[ cos\\theta = x'' \\cdot w' \\]\nこれにAdditive Angular Margin Penaltyを適用する．\nAdditive Angular Margin Penalty Additive Angular Margin Penaltyは正解ラベルに対応する出力の値に対して，Marginを加えることで，クラス内分散を小さくするような学習を行う． イメージとしては，正解ラベルにのみ厳しい罰則を与えてよりDiscriminativeにする感じ．\n正解クラス\\(j\\)の出力に対して，Marginを加算する\n\\[ \\theta_j' = \\{ \\begin{array}{ll} arccos(cos\\theta_i) + m \u0026 i=j \\\\ arccos(cos\\theta_i) \u0026 otherwise \\end{array} \\]\n各要素を定数倍する（温度パラメータ）\n\\[ logit = s cos(\\theta_j') \\]\nsoftmax関数にかける\n\\[ y = softmax(logit) \\]\nこの一連の流れを組み込んだLoss関数は\n\\[ L_{3}=-\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{e^{s\\left(\\cos \\left(\\theta_{y_{i}}+m\\right)\\right)}}{e^{s\\left(\\cos \\left(\\theta_{y_{i}}+m\\right)\\right)}+\\sum_{j=1, j \\neq y_{i}}^{n} e^{s \\cos \\theta_{j}}} \\]\nArcFace・SpehereFace・CosFase の一般化 \\(m_1\\)がSpehereFace，\\(m_2\\)がArcFace，\\(m_3\\)がCosFace．\n\\[ L_{4}=-\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{e^{s\\left(\\cos \\left(m_{1} \\theta_{y_{i}}+m_{2}\\right)-m_{3}\\right)}}{e^{s\\left(\\cos \\left(m_{1} \\theta_{y_{i}}+m_{2}\\right)-m_{3}\\right)}+\\sum_{j=1, j \\neq y_{i}}^{n} e^{s \\cos \\theta_{j}}} \\]\nそれぞれの識別境界の違いは下図になるらしい． 4. どうやって有効だと検証した？ 顔認識データセットであるLFW，CFP-FP，AgeDB30で実験． 比較手法がどれも精度が優秀なので，あまり有効さはわからない．\n5. 議論はあるか？  Out of dataset のサンプルが来た時にどれくらい精度がでるか？  6. 次に読むべき論文はある？  CosFace: Large Margin Cosine Loss for Deep Face Recognition https://arxiv.org/abs/1801.09414 SphereFace: Deep Hypersphere Embedding for Face Recognition https://arxiv.org/abs/1704.08063  ","date":1573916400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573916400,"objectID":"cbb19ff663abc875ed87ef1f97ea3bcf","permalink":"https://salty-vanilla.github.io/portfolio/post/arcface/","publishdate":"2019-11-17T00:00:00+09:00","relpermalink":"/portfolio/post/arcface/","section":"post","summary":"1. どんなもの？ Metric Learningの論文．分類をして，各クラス内の分散を小さく，クラス間の分散を大きくする系のMetric Learining．\n2. 先行研究と比べてどこがすごい？  クラス分類モデルのSoftmaxを少し改良するだけで適用できる ArcFaceと先行研究のSpehereFace・CosFaseのLoss関数は似ていて，それを一般化している  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Architecture 全体的な流れとしては， Base Block（VGGとかResNetとか）から特徴ベクトルを出力\n\\[ x' = f(x) \\]\n出力された特徴ベクトルをL2正則化\n\\[ x'' = \\frac{x'}{|x'|^2} \\]\n全結合層の重みをL2正則化\n\\[ w' = \\frac{w}{|w|^2} \\]\n正則化された特徴ベクトルと重みを内積（これがcosの値）\n\\[ cos\\theta = x'' \\cdot w' \\]\nこれにAdditive Angular Margin Penaltyを適用する．\nAdditive Angular Margin Penalty Additive Angular Margin Penaltyは正解ラベルに対応する出力の値に対して，Marginを加えることで，クラス内分散を小さくするような学習を行う． イメージとしては，正解ラベルにのみ厳しい罰則を与えてよりDiscriminativeにする感じ．\n正解クラス\\(j\\)の出力に対して，Marginを加算する\n\\[ \\theta_j' = \\{ \\begin{array}{ll} arccos(cos\\theta_i) + m \u0026 i=j \\\\ arccos(cos\\theta_i) \u0026 otherwise \\end{array} \\]","tags":null,"title":"ArcFace: Additive Angular Margin Loss for Deep Face Recognition","type":"post"},{"authors":null,"categories":null,"content":"1. どんなもの？ 異常検知の論文．Autoencoderの出力を複数にすることでAutoencoderの異常検知の問題を解決する．\n2. 先行研究と比べてどこがすごい？  Autoencoderの入出力による異常検知では，出力がぼやけてしまい高周波成分が再構成できず正常と異常のSN比が小さいという問題があった． 後述するMultiple-Hypothesesにより高周波成分の再構成に成功．  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Multiple-Hypotheses VAEのDecoderから得られる出力を複数にする． 具体的には，$H$個のDeconv Layerを最終層に配置し，それぞれ独立のパラメータで出力させる（事後分布はGaussian）． winner-takes-all (WTA) loss 複数のDecoderの出力に対して，全ておいて再構成誤差をBack Propagationするのではなく， 最も再構成誤差が低い出力(winner)のみから再構成誤差をBack Propagationさせる．\n\\[ \\begin{aligned} L_{W T A}\\left(x_{i} | \\theta_{h}\\right) \u0026=E_{z_{k} \\sim q_{\\phi}(z | x)}\\left[\\log p_{\\theta_{h}}\\left(x_{i} | z_{k}\\right)\\right] \\\\ \\text { s.t. } h \u0026=\\arg \\max _{j} E_{z_{k} \\sim q_{\\phi}(z | x)}\\left[\\log p_{\\theta_{j}}\\left(x_{i} | z_{k}\\right)\\right] \\end{aligned} \\]\nDiscriminator WTA Lossでは再構成誤差をBack Propagationする出力以外については更新がされないことになってしまう． そのため，それ以外の出力についても入力の分布に近づけるようにDiscriminatorを用意する． realはもちろん入力画像で，fakeはVAEの出力（Bestとそれ以外）とランダムサンプリングされた$z$からDecoderを介して得られた出力である．\n\\[ \\begin{aligned} \\min _{D} \\max _{G} L_{D}(x, z)=\u0026\\min _{D} \\max _{G} \\underbrace{-\\log \\left(p_{D}\\left(x_{r e a l}\\right)\\right)}_{L_{real}} +L_{f a k e}(x, z) \\end{aligned} \\]\n\\[ \\begin{array}{l}{L_{\\text {fake }}(x, z)=\\log \\left(p_{D}\\left(\\hat{x}_{z \\sim \\mathcal{N}(0,1)}\\right)\\right)} {+\\log \\left(p_{D}\\left(\\hat{x}_{z \\sim \\mathcal{N}}\\left(\\mu_{\\left.z | x, \\Sigma_{z | x}\\right)}\\right)\\right)+\\log \\left(p_{D}\\left(\\hat{x}_{\\text {best-guess }}\\right)\\right)\\right.}\\end{array} \\]\nVAEのLoss関数は，\n\\[ \\min _{G} L_{G}=\\min _{G} L_{W T A}+K L\\left(q_{\\phi}(z | x) \\| \\mathcal{N}(0,1)\\right)-L_{D} \\]\n異常度の算出 WTA Lossを異常度とする． Sumしなければ，異常箇所のLocalizationに使えるのは従来のAutoencoder通り．\n4. どうやって有効だと検証した？ CIFAR10(1vs9)とMETAL ANOMALY（論文内にはリンクなし）で実験． CIFAR10でAUROC: 67.1． METAL ANOMALYでは異常度が大きいPixelの上位10%のSumを全体の異常度として算出．\n5. 議論はあるか？  Blurが解消されたのは，VAE-GAN構造にしたことによるところが大きいと思うが果たして． 高周波成分が再構成されることにより，今まで差分として出てこなかった部分もあると思う．  6. 次に読むべき論文はある？ ","date":1572534000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572534000,"objectID":"ff3a913e37d04845f459625da0d43b59","permalink":"https://salty-vanilla.github.io/portfolio/post/anomaly_detection_with_multiple-hypotheses_predictions/","publishdate":"2019-11-01T00:00:00+09:00","relpermalink":"/portfolio/post/anomaly_detection_with_multiple-hypotheses_predictions/","section":"post","summary":"1. どんなもの？ 異常検知の論文．Autoencoderの出力を複数にすることでAutoencoderの異常検知の問題を解決する．\n2. 先行研究と比べてどこがすごい？  Autoencoderの入出力による異常検知では，出力がぼやけてしまい高周波成分が再構成できず正常と異常のSN比が小さいという問題があった． 後述するMultiple-Hypothesesにより高周波成分の再構成に成功．  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ Multiple-Hypotheses VAEのDecoderから得られる出力を複数にする． 具体的には，$H$個のDeconv Layerを最終層に配置し，それぞれ独立のパラメータで出力させる（事後分布はGaussian）． winner-takes-all (WTA) loss 複数のDecoderの出力に対して，全ておいて再構成誤差をBack Propagationするのではなく， 最も再構成誤差が低い出力(winner)のみから再構成誤差をBack Propagationさせる．\n\\[ \\begin{aligned} L_{W T A}\\left(x_{i} | \\theta_{h}\\right) \u0026=E_{z_{k} \\sim q_{\\phi}(z | x)}\\left[\\log p_{\\theta_{h}}\\left(x_{i} | z_{k}\\right)\\right] \\\\ \\text { s.t. } h \u0026=\\arg \\max _{j} E_{z_{k} \\sim q_{\\phi}(z | x)}\\left[\\log p_{\\theta_{j}}\\left(x_{i} | z_{k}\\right)\\right] \\end{aligned} \\]\nDiscriminator WTA Lossでは再構成誤差をBack Propagationする出力以外については更新がされないことになってしまう． そのため，それ以外の出力についても入力の分布に近づけるようにDiscriminatorを用意する． realはもちろん入力画像で，fakeはVAEの出力（Bestとそれ以外）とランダムサンプリングされた$z$からDecoderを介して得られた出力である．\n\\[ \\begin{aligned} \\min _{D} \\max _{G} L_{D}(x, z)=\u0026\\min _{D} \\max _{G} \\underbrace{-\\log \\left(p_{D}\\left(x_{r e a l}\\right)\\right)}_{L_{real}} +L_{f a k e}(x, z) \\end{aligned} \\]","tags":null,"title":"Anomaly Detection With Multiple-Hypotheses Predictions","type":"post"},{"authors":null,"categories":null,"content":"1. どんなもの？ 推論時に時間がかかってしまうAnoGANを高速化する枠組み．\n2. 先行研究と比べてどこがすごい？ AnoGANでは，推論時に$z$から$x$へのmappingを行うために学習済みGANのDiscriminatorの結果と再構成誤差からLossを算出し，勾配降下法によって$z$を探索していた． つまり，推論時にも”学習”のフェーズが存在し処理時間が長かった．\nf-AnoGANでは，推論時の勾配降下による探索を無くし，推論の高速化を行った．\n3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ $z$ から$x$を推論する枠組みを3つ提案．\nziz encoder 学習済みのGANのGeneratorを用いて，$z$をGeneratorに入力し，その出力をziz encoderに入力し得られた潜在ベクトルとの再構成誤差を最小化する．\n$n$は総画素数． $$ L(z) = \\frac{1}{n}|z - E(G(z))|^2 $$\nizi encoder 学習済みのGANのGeneratorを用いて，$x$をEncoderに入力し，その出力をizi encoderに入力し得られた画像との再構成誤差を最小化する． $$ L(x) = \\frac{1}{n}|x - G(E(x))|^2 $$\nizif encoder izi encoderの派生形で，izi encoderのLossと同様の再構成誤差と，Discriminatorに$x$と$G(E(x))$を入力した際の中間層の出力の再構成誤差の和を最小化する． $f(\\cdot)$はDiscriminatorの中間層の出力で，$n_d$は$f(\\cdot)$の次元数で$k$は重みパラメータ．． $$ L(x) = \\frac{1}{n}|x - G(E(x))|^2 + \\frac{k}{n_d}|f(x)-f(G(E(x)))|^2 $$\n異常度の算出 $$ A(x) = \\frac{1}{n}|x - G(E(x))|^2 + \\frac{k}{n_d}|f(x)-f(G(E(x)))|^2 $$\n4. どうやって有効だと検証した？ AnoGANと同様にretinal spectral-domain optical coherence tomography (SD-OCT)をデータセットとして実験． Autoencoder，AAE，ALI，WGANのDiscriminator，iterative(AnoGAN)と比較して精度も上回った．\n5. 議論はあるか？ 追加のEncoderをつけるという簡単な手法で高速化＆高精度化を果たした点がGood． 構成的にはGANomalyに近い感じがするが，精度比較のほどは果たして？\n6. 次に読むべき論文はある？  AnoGAN https://arxiv.org/abs/1703.05921 GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training https://arxiv.org/abs/1805.06725 Adversarially Learned Inference https://arxiv.org/abs/1606.00704  ","date":1570978800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570978800,"objectID":"dbf2807c36d84c7a0f8affaefadded0c","permalink":"https://salty-vanilla.github.io/portfolio/post/f-anogan/","publishdate":"2019-10-14T00:00:00+09:00","relpermalink":"/portfolio/post/f-anogan/","section":"post","summary":"1. どんなもの？ 推論時に時間がかかってしまうAnoGANを高速化する枠組み．\n2. 先行研究と比べてどこがすごい？ AnoGANでは，推論時に$z$から$x$へのmappingを行うために学習済みGANのDiscriminatorの結果と再構成誤差からLossを算出し，勾配降下法によって$z$を探索していた． つまり，推論時にも”学習”のフェーズが存在し処理時間が長かった．\nf-AnoGANでは，推論時の勾配降下による探索を無くし，推論の高速化を行った．\n3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ $z$ から$x$を推論する枠組みを3つ提案．\nziz encoder 学習済みのGANのGeneratorを用いて，$z$をGeneratorに入力し，その出力をziz encoderに入力し得られた潜在ベクトルとの再構成誤差を最小化する．\n$n$は総画素数． $$ L(z) = \\frac{1}{n}|z - E(G(z))|^2 $$\nizi encoder 学習済みのGANのGeneratorを用いて，$x$をEncoderに入力し，その出力をizi encoderに入力し得られた画像との再構成誤差を最小化する． $$ L(x) = \\frac{1}{n}|x - G(E(x))|^2 $$\nizif encoder izi encoderの派生形で，izi encoderのLossと同様の再構成誤差と，Discriminatorに$x$と$G(E(x))$を入力した際の中間層の出力の再構成誤差の和を最小化する． $f(\\cdot)$はDiscriminatorの中間層の出力で，$n_d$は$f(\\cdot)$の次元数で$k$は重みパラメータ．． $$ L(x) = \\frac{1}{n}|x - G(E(x))|^2 + \\frac{k}{n_d}|f(x)-f(G(E(x)))|^2 $$\n異常度の算出 $$ A(x) = \\frac{1}{n}|x - G(E(x))|^2 + \\frac{k}{n_d}|f(x)-f(G(E(x)))|^2 $$\n4. どうやって有効だと検証した？ AnoGANと同様にretinal spectral-domain optical coherence tomography (SD-OCT)をデータセットとして実験． Autoencoder，AAE，ALI，WGANのDiscriminator，iterative(AnoGAN)と比較して精度も上回った．\n5. 議論はあるか？ 追加のEncoderをつけるという簡単な手法で高速化＆高精度化を果たした点がGood． 構成的にはGANomalyに近い感じがするが，精度比較のほどは果たして？","tags":null,"title":"f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks","type":"post"},{"authors":null,"categories":null,"content":"背景 手法 ","date":1556809200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556809200,"objectID":"816dc8a0372121d1b0260f07e0230f4d","permalink":"https://salty-vanilla.github.io/portfolio/project/wlid_life/","publishdate":"2019-05-03T00:00:00+09:00","relpermalink":"/portfolio/project/wlid_life/","section":"project","summary":"大量のカメラトラップ画像から野生動物が何頭いるかを自動判定","tags":null,"title":"Wild Life","type":"project"},{"authors":null,"categories":null,"content":"1. どんなもの？ Attention Mapを使ってCNNが分類を行うときに使う有効な視覚的情報の空間的なサポートを見つけ出し，利用することで一般物体認識の精度を向上させる．\n2. 先行研究と比べてどこがすごい？  Saliency Mapを用いることで有効な領域の情報を重視し，無関係な情報を抑制する Local feature vector (CNNの中間層の出力)とGlobal feature vector (CNNの後段のFCの出力)を組み合わせる 適合度によって重要なLocal feature vectorだけを分類に活用する  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ 学習可能なAttention Estimatorを通常のCNNに付け加えるだけで，Attention Mapによる解釈性，精度の向上．\n  $S$個のAttention Moduleを↑のようにCNNに加える．$s$個目のAttention Moduleは，長さ$M$のベクトル$N$個からなる集合である．\n  $s$個目のlocal feature vectorは $$ \\mathbf{L^s} = { \\mathbf{l_1^s}, \\mathbf{l_2^s}, \u0026hellip;, \\mathbf{l_N^s} } $$ ここで，ベクトルの長さ$M$はFeature Mapのチャネル数に等しく，ベクトルの個数$N$はFeature Mapの画素数に等しい．\n  全結合層で各ベクトルの長さをglobal feature vector $\\mathbf{g}$の長さ$M'$に揃える $$ \\mathbf{\\hat{l^s_i}} = w\\cdot{\\mathbf{l_i^s}} $$\n  local feature vectorとglobal feature vectorから各画素のCompatibility scoresを求める $$ C^s(\\mathbf{\\hat{L_s}}, \\mathbf{g}) = {c_1^s, c_2^s, \u0026hellip;, c_n^s} $$ $$ c_i^s = \\mathbf{\\hat{l^s_i}} \\cdot{\\mathbf{g}} $$\n  Compatibility scoresに対して，softmaxを適用してAttention Mapを算出 $$ a_i^s = \\frac{exp(c_i^s)}{\\sum_j^N exp(c_j^s)} $$\n  各モジュールの出力はAttention MapとFeature Mapの内積 $$ \\mathbf{g^s} = \\sum_i^n a_i^s \\cdot{\\mathbf{l_i^s}} $$\n  最終的には，全Moduleの出力を連結することでModule全体の出力として，最後にFC層\n  $$ \\mathbf{g_a} = { \\mathbf{g_1}, \\mathbf{g_2}, \u0026hellip;, \\mathbf{g_S}} $$ $$ O = W \\cdot{\\mathbf{g_a}} $$\n4. どうやって有効だと検証した？ CIFAR10，CIFAR100，CUB200，SVHNで実験． BaselineであるVGG，VGG+GAP, VGG+PAN, ResNet164と比較して精度向上． 浅い層では局所的な情報を重視し，深い層では物体全体の情報を重視していることがわかる\n5. 議論はあるか？ Adversarial AttackやCross Domainな認識タスクに対しても有効であることが示されている．\n6. 次に読むべき論文はある？ ","date":1525532400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525532400,"objectID":"aa74104a1b07b41903f5c46506262534","permalink":"https://salty-vanilla.github.io/portfolio/post/learn_to_pay_attention/","publishdate":"2018-05-06T00:00:00+09:00","relpermalink":"/portfolio/post/learn_to_pay_attention/","section":"post","summary":"1. どんなもの？ Attention Mapを使ってCNNが分類を行うときに使う有効な視覚的情報の空間的なサポートを見つけ出し，利用することで一般物体認識の精度を向上させる．\n2. 先行研究と比べてどこがすごい？  Saliency Mapを用いることで有効な領域の情報を重視し，無関係な情報を抑制する Local feature vector (CNNの中間層の出力)とGlobal feature vector (CNNの後段のFCの出力)を組み合わせる 適合度によって重要なLocal feature vectorだけを分類に活用する  3. 技術や手法の\u0026quot;キモ\u0026quot;はどこ？ 学習可能なAttention Estimatorを通常のCNNに付け加えるだけで，Attention Mapによる解釈性，精度の向上．\n  $S$個のAttention Moduleを↑のようにCNNに加える．$s$個目のAttention Moduleは，長さ$M$のベクトル$N$個からなる集合である．\n  $s$個目のlocal feature vectorは $$ \\mathbf{L^s} = { \\mathbf{l_1^s}, \\mathbf{l_2^s}, \u0026hellip;, \\mathbf{l_N^s} } $$ ここで，ベクトルの長さ$M$はFeature Mapのチャネル数に等しく，ベクトルの個数$N$はFeature Mapの画素数に等しい．\n  全結合層で各ベクトルの長さをglobal feature vector $\\mathbf{g}$の長さ$M'$に揃える $$ \\mathbf{\\hat{l^s_i}} = w\\cdot{\\mathbf{l_i^s}} $$\n  local feature vectorとglobal feature vectorから各画素のCompatibility scoresを求める $$ C^s(\\mathbf{\\hat{L_s}}, \\mathbf{g}) = {c_1^s, c_2^s, \u0026hellip;, c_n^s} $$ $$ c_i^s = \\mathbf{\\hat{l^s_i}} \\cdot{\\mathbf{g}} $$","tags":null,"title":"Learn to Pay Attention","type":"post"},{"authors":null,"categories":null,"content":"国内会議  中塚俊介, 加藤邦人, 中西洋輔 : \u0026ldquo;CNNによる回帰分析を用いた打痕判定に関する考察\u0026rdquo;, ビジョン技術の実利用ワークショップ ViEW2016, pp.204-205(2016.12.9) 中塚俊介, 加藤邦人, 中西洋輔 : \u0026ldquo;回帰型CNNを用いた工業製品における外観検査手法の研究\u0026rdquo;, 第22回知能メカトロニクスワークショップ, 3A1-4(2017.8.28) 神本恭佑, 中塚俊介, 相澤宏旭, 加藤邦人, 小林裕幸, 坂野和見 : \u0026ldquo;Denoising Autoencoder Generative Adversarial Networks を用いた欠損検出の検討\u0026rdquo;, ビジョン技術の実利用ワークショップ ViEW2017, pp.54-55(2017.12.7) 中塚俊介, 相澤宏旭, 加藤邦人 : \u0026ldquo;少数不良品サンプル下におけるAdversarial AutoEncoderによる正常モデルの生成と不良判別\u0026rdquo;, ビジョン技術の実利用ワークショップ ViEW2017, pp.148-149(2017.12.8) 安藤正規，中塚俊介，相澤宏旭，中森さつき，池田敬，森部絢嗣，寺田和憲，加藤邦人: \u0026ldquo;機械学習による自動撮影カメラ画像からの獣種自動判別技術の開発\u0026rdquo;，日本哺乳類学会2018年度大会，S2-05，(2018.9)  国際会議  Shunsuke Nakatsuka, Kunihito Kato, Yosuke Nakanishi : \u0026ldquo;Study on Visual Inspection Method using CNN Regression\u0026rdquo;, Asia International Symposium on Mechatronics, D1-5(2017.9.15) Kyosuke Komoto, Shunsuke Nakatsuka Hiroaki, Aizawa, Kunihito Kato, Hiroyuki Kobayashi, Kazumi Banno : \u0026ldquo;A Performance Evaluation of Defect Detection by using Denoising AutoEncoder Generative Adversarial Networks\u0026rdquo;, International Workshop on Advanced Image Technology 2018, Session E2-4 (2018.1.9) Shunsuke Nakatsuka, Hiroaki Aizawa and Kunihito Kato : \u0026ldquo;A Method of Generation of Normal Model and Discrimination of Defects by Adversarial AutoEncoder under Small Number of Defective Samples\u0026rdquo;, Proceeding of 24rd International Workshop on Frontiers of Computer Vision, OS3-1,(2018.2.22)  論文誌  中塚俊介，相澤宏旭，加藤邦人：\u0026ldquo;少数不良品サンプル下におけるAdversarial AutoEncoderによる正常モデルの生成と異常検出\u0026rdquo;，精密工学会誌，投稿中 安藤正規，中塚俊介，相澤宏旭，中森さつき，池田敬，森部絢嗣，寺田和憲，加藤邦人: \u0026ldquo;深層学習（Deep Learning）によるカメラトラップ画像の判別\u0026rdquo;，哺乳類科学, 投稿中  雑誌  中塚俊介, 相澤宏旭, 加藤邦人 : \u0026ldquo;少数不良品サンプル下におけるAdversarial AutoEncoderによる正常モデルの生成と不良判別\u0026rdquo;, 映像情報インダストリアル, pp.57-68(2018.03)  特許  加藤邦人, 中塚俊介, 相澤宏旭 : \u0026ldquo;異常品判定方法\u0026rdquo; (特願2017-196758/2017.10.10出願)  受賞  Best Paper Award受賞 Shunsuke Nakatsuka, Kunihito Kato, Yosuke Nakanishi : \u0026ldquo;Study on Visual Inspection Method using CNN Regression\u0026rdquo;, Asia International Symposium on Mechatronics, D1-5(2017年9月15日受賞) ViEW2017 ビジョン技術の実利用ワークショップ 小田原賞（優秀論文賞）, 中塚俊介, 相澤宏旭, 加藤邦人 : \u0026ldquo;少数不良品サンプル下におけるAdversarial AutoEncoderによる正常モデルの生成と不良判別\u0026rdquo;, ビジョン技術の実利用ワークショップ ViEW2017, pp.148-149 (2017年12月8日 受賞)  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40ea366a28f9524de71378c3212c5489","permalink":"https://salty-vanilla.github.io/portfolio/publication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/portfolio/publication/","section":"","summary":"国内会議  中塚俊介, 加藤邦人, 中西洋輔 : \u0026ldquo;CNNによる回帰分析を用いた打痕判定に関する考察\u0026rdquo;, ビジョン技術の実利用ワークショップ ViEW2016, pp.204-205(2016.12.9) 中塚俊介, 加藤邦人, 中西洋輔 : \u0026ldquo;回帰型CNNを用いた工業製品における外観検査手法の研究\u0026rdquo;, 第22回知能メカトロニクスワークショップ, 3A1-4(2017.8.28) 神本恭佑, 中塚俊介, 相澤宏旭, 加藤邦人, 小林裕幸, 坂野和見 : \u0026ldquo;Denoising Autoencoder Generative Adversarial Networks を用いた欠損検出の検討\u0026rdquo;, ビジョン技術の実利用ワークショップ ViEW2017, pp.54-55(2017.12.7) 中塚俊介, 相澤宏旭, 加藤邦人 : \u0026ldquo;少数不良品サンプル下におけるAdversarial AutoEncoderによる正常モデルの生成と不良判別\u0026rdquo;, ビジョン技術の実利用ワークショップ ViEW2017, pp.148-149(2017.12.8) 安藤正規，中塚俊介，相澤宏旭，中森さつき，池田敬，森部絢嗣，寺田和憲，加藤邦人: \u0026ldquo;機械学習による自動撮影カメラ画像からの獣種自動判別技術の開発\u0026rdquo;，日本哺乳類学会2018年度大会，S2-05，(2018.9)  国際会議  Shunsuke Nakatsuka, Kunihito Kato, Yosuke Nakanishi : \u0026ldquo;Study on Visual Inspection Method using CNN Regression\u0026rdquo;, Asia International Symposium on Mechatronics, D1-5(2017.9.15) Kyosuke Komoto, Shunsuke Nakatsuka Hiroaki, Aizawa, Kunihito Kato, Hiroyuki Kobayashi, Kazumi Banno : \u0026ldquo;A Performance Evaluation of Defect Detection by using Denoising AutoEncoder Generative Adversarial Networks\u0026rdquo;, International Workshop on Advanced Image Technology 2018, Session E2-4 (2018.","tags":null,"title":"Publications","type":"page"}]